{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, precision_recall_curve\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats import f_oneway"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THIS IS AN EXAMPLE AND PUT AT TOP OF FILE\n",
    "Problem Statement:\n",
    "Objective:\n",
    "The objective of this analysis is to develop a predictive model capable of accurately forecasting the delivery status of orders within the supply chain. The \"Delivery Status\" column will serve as the target variable for the model. Through the implementation of diverse machine learning algorithms, the intention is to discern underlying patterns and trends in the data, thereby gaining insights into the determinants of delayed deliveries and enhancing the overall efficiency of the supply chain.\n",
    "Â \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Hypothesis:\n",
    "Our hypothesis posits that specific features in the dataset, namely \"Scheduled Days for shipping,\" \"Shipping Mode,\" \"Order Region\", \"Payment type\",\"Order Item quantity\", \"order date\" and \"shipping date\" exert a substantial influence on the delivery status of orders. \n",
    "Additionally, we expect that certain customer categories or segments may exhibit higher susceptibility to experiencing late deliveries. Moreover, we anticipate that both the chosen shipping mode and delivery location will have an impact on the order delivery time. Through the utilization of these identified insights, we can construct a model with the capability to accurately forecast delivery status and provide actionable recommendations for optimizing the supply chain process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the file midterm_data from downloads - REPLACE THIS WITH PROPER DATASET\n",
    "df = pd.read_csv(R'C:\\Users\\hanna\\Downloads\\midterm_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use head() function to quickly assess the data set\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see several things by using the head() function: REPLACE THIS WITH RELEVANT OBSERVATIONS\n",
    "- We have a numerical column describing the user ID associated to each person and another numerical column describing their age\n",
    "- There is another numerical column called sessions, which is the session ID for each user\n",
    "- We have a numerical time_spent column, which is the number of minutes they spent browsing, a numerical column pages_visited describing the number of pages they visited during their session, and another numerical column cart_items describing the number of items in their cart\n",
    "- The next numerical column is cart_value, which lists the value of the items in each user's cart\n",
    "- Checkout status lists a 1 if they did check out and a 0 if they did not\n",
    "- Finally, there is a categorical column describing the device type and another categorical column describing the location of the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next use info() function to assess if there are structural issues\n",
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see several things using the info() function: REPLACE THIS WITH RELEVANT OBSERVATIONS\n",
    "- There are 5000 entries\n",
    "- Many of the columns do not have missing entries, with the exception of the device and location columns\n",
    "- Device has (5000 - 4900) 100 missing entries, or about 2% missing data\n",
    "- Location has (5000 - 4970) missing entries, or about 0.4% missing data\n",
    "- These are relatively small amounts of missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next, use the describe() function for some initial descriptive statistics\n",
    "#include = all because there is categorical data\n",
    "df.describe(include = 'all')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see several things using the describe() function: REPLACE THIS WITH RELEVANT OBSERVATIONS\n",
    "- The mean age of users is 42 (rounded) which is the same as the median age of 42. This indicates a normal distribution\n",
    "- The mean time spent (25) and median time spent (25) are the same, also indicating normal distribution\n",
    "- Mean/median for pages visited and cart items are also about the same\n",
    "- The mean cart value is $149.44 while the median is 143.44, which may indicate a minimal amount of skewness\n",
    "- There are 3 unique device types and 5 unique locations\n",
    "- The most common device type is a desktop and the most common location of users is location 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MOVE ON TO DATA CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if there are unwanted columns, drop them with this code\n",
    "# List of columns to drop\n",
    "columns_to_drop = ['Customer Zipcode','Customer Password','Benefit per order','Category Id', 'Category Name','Customer City',\n",
    "       'Customer Country', 'Customer Id', 'Customer Segment', 'Customer State',\n",
    "                   'Department Id','Order Zipcode','Customer Street'\n",
    "                   ,'Product Category Id',\n",
    "                   'Customer Fname','Customer Lname',\n",
    "                   'Order City']\n",
    "\n",
    "# Dropping the specified columns from the DataFrame\n",
    "df= df.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if want to check if rows have missing data in two or more columns, use this code\n",
    "df[df['device'].isnull() & df['location'].isnull()]\n",
    "\n",
    "#if you get results, use this to drop the rows\n",
    "df = df.drop(df[df['userID'] == 2131].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can use this to check data statistics again and make sure mean, median, mode have not changed too much\n",
    "df.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this gives the sum of null values in every column\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if there is not too many, can use this to drop the values\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#can use this again to make sure there are no more null values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use this to fill in missing values in a column with 'Other'\n",
    "df['device'] = df['device'].fillna('Other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this creates a boxplot to check for outliers in certain columns\n",
    "sns.boxplot(data = df, y='time_spent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is used to drop the outliers if there does not appear to be many\n",
    "df.drop(df[df['time_spent'] > 80].index, axis = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lastly, drop duplicates if needed\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure not too many observations have been dropped, <10% of observations\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations and Descriptive Statistics\n",
    "# You can create various visualizations to understand the data\n",
    "sns.pairplot(df)  # Create pairwise scatter plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "# Feature Transformation and Scaling\n",
    "# Assuming you have numerical and categorical features\n",
    "num_features = ['feature1', 'feature2', 'feature3']\n",
    "cat_features = ['categorical_feature']\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), num_features),\n",
    "        ('cat', OneHotEncoder(), cat_features)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Development\n",
    "# Split the data into train and test sets\n",
    "X = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "models = [\n",
    "    ('Naive Bayes', GaussianNB()),\n",
    "    ('K-Nearest Neighbors', KNeighborsClassifier()),\n",
    "    ('Logistic Regression', LogisticRegression()),\n",
    "    ('Support Vector Machine', SVC()),\n",
    "    ('Decision Tree', DecisionTreeClassifier()),\n",
    "    ('Random Forest', RandomForestClassifier())\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through models, train, and evaluate\n",
    "results = []\n",
    "for name, model in models:\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('model', model)])\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    results.append((name, accuracy, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display model evaluation results\n",
    "evaluation_df = pd.DataFrame(results, columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n",
    "print(evaluation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "# You can also evaluate ROC curve and other metrics\n",
    "for name, model in models:\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('model', model)])\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    y_prob = pipeline.predict_proba(X_test)[:, 1]  # Probability of positive class\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'{name} (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional Analysis (if needed)\n",
    "# You can perform further statistical tests or analysis using f_oneway or other methods\n",
    "# Example:\n",
    "# groups = df['group_column'].unique()\n",
    "# anova_results = {}\n",
    "# for group in groups:\n",
    "#     data = df[df['group_column'] == group]['feature_to_compare']\n",
    "#     anova_results[group] = data\n",
    "# f_statistic, p_value = f_oneway(*anova_results.values())\n",
    "# print(f'F-statistic: {f_statistic}, p-value: {p_value}')\n",
    "\n",
    "# Select the best model based on evaluation metrics\n",
    "# You can use the evaluation results to decide on the best model for your problem\n",
    "\n",
    "# Final Thoughts\n",
    "# Interpret the results and draw conclusions about the problem statement and hypothesis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
