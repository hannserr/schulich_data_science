{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, precision_recall_curve\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats import f_oneway"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THIS IS AN EXAMPLE AND PUT AT TOP OF FILE\n",
    "Problem Statement:\n",
    "Objective:\n",
    "The objective of this analysis is to develop a predictive model capable of accurately forecasting the delivery status of orders within the supply chain. The \"Delivery Status\" column will serve as the target variable for the model. Through the implementation of diverse machine learning algorithms, the intention is to discern underlying patterns and trends in the data, thereby gaining insights into the determinants of delayed deliveries and enhancing the overall efficiency of the supply chain.\n",
    "Â \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Hypothesis:\n",
    "Our hypothesis posits that specific features in the dataset, namely \"Scheduled Days for shipping,\" \"Shipping Mode,\" \"Order Region\", \"Payment type\",\"Order Item quantity\", \"order date\" and \"shipping date\" exert a substantial influence on the delivery status of orders. \n",
    "Additionally, we expect that certain customer categories or segments may exhibit higher susceptibility to experiencing late deliveries. Moreover, we anticipate that both the chosen shipping mode and delivery location will have an impact on the order delivery time. Through the utilization of these identified insights, we can construct a model with the capability to accurately forecast delivery status and provide actionable recommendations for optimizing the supply chain process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the file midterm_data from downloads - REPLACE THIS WITH PROPER DATASET\n",
    "df = pd.read_csv(R'C:\\Users\\hanna\\Downloads\\midterm_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use head() function to quickly assess the data set\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see several things by using the head() function: REPLACE THIS WITH RELEVANT OBSERVATIONS\n",
    "- We have a numerical column describing the user ID associated to each person and another numerical column describing their age\n",
    "- There is another numerical column called sessions, which is the session ID for each user\n",
    "- We have a numerical time_spent column, which is the number of minutes they spent browsing, a numerical column pages_visited describing the number of pages they visited during their session, and another numerical column cart_items describing the number of items in their cart\n",
    "- The next numerical column is cart_value, which lists the value of the items in each user's cart\n",
    "- Checkout status lists a 1 if they did check out and a 0 if they did not\n",
    "- Finally, there is a categorical column describing the device type and another categorical column describing the location of the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next use info() function to assess if there are structural issues\n",
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see several things using the info() function: REPLACE THIS WITH RELEVANT OBSERVATIONS\n",
    "- There are 5000 entries\n",
    "- Many of the columns do not have missing entries, with the exception of the device and location columns\n",
    "- Device has (5000 - 4900) 100 missing entries, or about 2% missing data\n",
    "- Location has (5000 - 4970) missing entries, or about 0.4% missing data\n",
    "- These are relatively small amounts of missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next, use the describe() function for some initial descriptive statistics\n",
    "#include = all because there is categorical data\n",
    "df.describe(include = 'all')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see several things using the describe() function: REPLACE THIS WITH RELEVANT OBSERVATIONS\n",
    "- The mean age of users is 42 (rounded) which is the same as the median age of 42. This indicates a normal distribution\n",
    "- The mean time spent (25) and median time spent (25) are the same, also indicating normal distribution\n",
    "- Mean/median for pages visited and cart items are also about the same\n",
    "- The mean cart value is $149.44 while the median is 143.44, which may indicate a minimal amount of skewness\n",
    "- There are 3 unique device types and 5 unique locations\n",
    "- The most common device type is a desktop and the most common location of users is location 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MOVE ON TO DATA CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if there are unwanted columns, drop them with this code\n",
    "# List of columns to drop\n",
    "columns_to_drop = ['Customer Zipcode','Customer Password','Benefit per order','Category Id', 'Category Name','Customer City',\n",
    "       'Customer Country', 'Customer Id', 'Customer Segment', 'Customer State',\n",
    "                   'Department Id','Order Zipcode','Customer Street'\n",
    "                   ,'Product Category Id',\n",
    "                   'Customer Fname','Customer Lname',\n",
    "                   'Order City']\n",
    "\n",
    "# Dropping the specified columns from the DataFrame\n",
    "df= df.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if want to check if rows have missing data in two or more columns, use this code\n",
    "df[df['device'].isnull() & df['location'].isnull()]\n",
    "\n",
    "#if you get results, use this to drop the rows\n",
    "df = df.drop(df[df['userID'] == 2131].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can use this to check data statistics again and make sure mean, median, mode have not changed too much\n",
    "df.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this gives the sum of null values in every column\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if there is not too many, can use this to drop the values\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#can use this again to make sure there are no more null values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use this to fill in missing values in a column with 'Other'\n",
    "df['device'] = df['device'].fillna('Other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this creates a boxplot to check for outliers in certain columns\n",
    "sns.boxplot(data = df, y='time_spent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is used to drop the outliers if there does not appear to be many\n",
    "df.drop(df[df['time_spent'] > 80].index, axis = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lastly, drop duplicates if needed\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure not too many observations have been dropped, <10% of observations\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations and Descriptive Statistics\n",
    "# You can create various visualizations to understand the data\n",
    "sns.pairplot(df)  # Create pairwise scatter plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "# Feature Transformation and Scaling\n",
    "# Assuming you have numerical and categorical features\n",
    "num_features = ['feature1', 'feature2', 'feature3']\n",
    "cat_features = ['categorical_feature']\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), num_features),\n",
    "        ('cat', OneHotEncoder(), cat_features)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Development\n",
    "# Split the data into train and test sets\n",
    "X = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "models = [\n",
    "    ('Naive Bayes', GaussianNB()),\n",
    "    ('K-Nearest Neighbors', KNeighborsClassifier()),\n",
    "    ('Logistic Regression', LogisticRegression()),\n",
    "    ('Support Vector Machine', SVC()),\n",
    "    ('Decision Tree', DecisionTreeClassifier()),\n",
    "    ('Random Forest', RandomForestClassifier())\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through models, train, and evaluate\n",
    "results = []\n",
    "for name, model in models:\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('model', model)])\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    results.append((name, accuracy, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display model evaluation results\n",
    "evaluation_df = pd.DataFrame(results, columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n",
    "print(evaluation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "# You can also evaluate ROC curve and other metrics\n",
    "for name, model in models:\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('model', model)])\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    y_prob = pipeline.predict_proba(X_test)[:, 1]  # Probability of positive class\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'{name} (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional Analysis (if needed)\n",
    "# You can perform further statistical tests or analysis using f_oneway or other methods\n",
    "# Example:\n",
    "# groups = df['group_column'].unique()\n",
    "# anova_results = {}\n",
    "# for group in groups:\n",
    "#     data = df[df['group_column'] == group]['feature_to_compare']\n",
    "#     anova_results[group] = data\n",
    "# f_statistic, p_value = f_oneway(*anova_results.values())\n",
    "# print(f'F-statistic: {f_statistic}, p-value: {p_value}')\n",
    "\n",
    "# Select the best model based on evaluation metrics\n",
    "# You can use the evaluation results to decide on the best model for your problem\n",
    "\n",
    "# Final Thoughts\n",
    "# Interpret the results and draw conclusions about the problem statement and hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 1. Import data\n",
    "# Replace 'your_dataset.csv' with the actual path to your dataset\n",
    "df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# 2. Dataframe checks\n",
    "print(df.info())\n",
    "print(df.describe())\n",
    "\n",
    "# 3. Data cleaning\n",
    "# Clean missing values\n",
    "missing_percentage = df.isnull().mean() * 100\n",
    "cols_to_drop = missing_percentage[missing_percentage > 10].index\n",
    "cols_to_fill = missing_percentage[missing_percentage <= 10].index\n",
    "\n",
    "df_cleaned = df.drop(cols_to_drop, axis=1)\n",
    "df_cleaned[cols_to_fill] = df_cleaned[cols_to_fill].fillna(df_cleaned[cols_to_fill].median())\n",
    "\n",
    "# 4. Data exploration\n",
    "# ... (Exploratory analysis and visualization for each variable)\n",
    "\n",
    "# 5. Feature engineering\n",
    "# ... (Feature engineering, binning, dummy variables, etc.)\n",
    "\n",
    "# 6. Data model\n",
    "# Assuming 'X' contains your feature columns and 'y' contains your target variable\n",
    "X = df_cleaned.drop('target_column_name', axis=1)\n",
    "y = df_cleaned['target_column_name']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'SVM': SVC(),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier()\n",
    "}\n",
    "\n",
    "# Cross-validation and model selection\n",
    "results = {}\n",
    "for model_name, model in models.items():\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=10, scoring='f1')\n",
    "    results[model_name] = cv_scores.mean()\n",
    "\n",
    "best_model = max(results, key=results.get)\n",
    "selected_model = models[best_model]\n",
    "\n",
    "# Hyperparameter tuning using RandomizedSearchCV\n",
    "param_dist = {\n",
    "    # Define hyperparameter ranges for your selected model\n",
    "}\n",
    "#example\n",
    "param_dist = {\n",
    "    'SVM__C': [0.1, 1, 10],  # Example: tuning C parameter for SVM\n",
    "    'Random Forest__n_estimators': [50, 100, 150],  # Example: tuning n_estimators for Random Forest\n",
    "    # Add more hyperparameters as needed for each model\n",
    "}\n",
    "\n",
    "\n",
    "random_search = RandomizedSearchCV(selected_model, param_distributions=param_dist, n_iter=100, cv=5, scoring='f1', random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "best_tuned_model = random_search.best_estimator_\n",
    "\n",
    "# Model evaluation\n",
    "y_pred = best_tuned_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# 7. Explaining your results\n",
    "# ... (Explanation of your process, choices, and results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Import the dataset\n",
    "df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Exploratory Analysis\n",
    "print(df.info())\n",
    "print(df.describe())\n",
    "\n",
    "# Explore conversion rate by demographic and behavioral variables\n",
    "conversion_by_gender = df.groupby('gender')['converted_Fri'].mean()\n",
    "conversion_by_channel = df.groupby('marketing_channel')['converted_Fri'].mean()\n",
    "# ... more exploratory analysis ...\n",
    "\n",
    "# Data Preparation\n",
    "# Drop irrelevant columns and handle missing values\n",
    "# ...\n",
    "\n",
    "# Feature Engineering\n",
    "# Calculate average ratings and prices for the week\n",
    "df['avg_rating_week'] = (df['avg_rating_Mon'] + df['avg_rating_Tue'] +\n",
    "                         df['avg_rating_Wed'] + df['avg_rating_Thu'] + df['avg_rating_Fri']) / 5\n",
    "df['avg_price_week'] = (df['avg_price_Mon'] + df['avg_price_Tue'] +\n",
    "                        df['avg_price_Wed'] + df['avg_price_Thu'] + df['avg_price_Fri']) / 5\n",
    "\n",
    "# Model Building\n",
    "# Prepare data for modeling\n",
    "X = df.drop(['visitor_id', 'converted_Fri'], axis=1)\n",
    "y = df['converted_Fri']\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define preprocessing steps for different types of columns\n",
    "numeric_features = ['product_views_Mon', 'product_views_Tue', 'product_views_Wed', 'product_views_Thu',\n",
    "                    'product_views_Fri', 'time_on_site_Mon', 'time_on_site_Tue', 'time_on_site_Wed',\n",
    "                    'time_on_site_Thu', 'time_on_site_Fri', 'pages_visited_Mon', 'pages_visited_Tue',\n",
    "                    'pages_visited_Wed', 'pages_visited_Thu', 'pages_visited_Fri', 'avg_rating_week', 'avg_price_week']\n",
    "categorical_features = ['visitor_location', 'marketing_channel', 'gender']\n",
    "\n",
    "numeric_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Build a Random Forest model\n",
    "model = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                        ('classifier', RandomForestClassifier(random_state=42))])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Model Evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Explanation to Business Executive\n",
    "# ...\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['views_to_ratings_ratio'] = df['product_views_week'] / df['avg_rating_week']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['total_time_spent_week'] = df['time_on_site_Mon'] + df['time_on_site_Tue'] +\n",
    "                              df['time_on_site_Wed'] + df['time_on_site_Thu'] + df['time_on_site_Fri']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['product_views_change'] = df['product_views_Fri'] - df['product_views_Mon']\n",
    "df['avg_rating_change'] = df['avg_rating_Fri'] - df['avg_rating_Mon']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['avg_pages_visited_per_day'] = (df['pages_visited_Mon'] + df['pages_visited_Tue'] +\n",
    "                                   df['pages_visited_Wed'] + df['pages_visited_Thu'] + df['pages_visited_Fri']) / 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['interacted_with_facebook'] = df['marketing_channel'].apply(lambda x: 1 if x == 'Facebook' else 0)\n",
    "df['interacted_with_google'] = df['marketing_channel'].apply(lambda x: 1 if x == 'Google' else 0)\n",
    "# Repeat for other marketing channels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gender_views_ratio'] = df['gender'].apply(lambda x: 1 if x == 'M' else 0) * df['product_views_week']\n",
    "# Repeat for other demographic attributes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pandas get_dummies() function\n",
    "df_dummies = pd.get_dummies(df, columns=['visitor_location'], drop_first=True)\n",
    "\n",
    "# Resulting columns: 'USA', 'Canada', 'UK'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pandas get_dummies() function\n",
    "df_dummies = pd.get_dummies(df, columns=['marketing_channel'], drop_first=True)\n",
    "\n",
    "# Resulting columns: 'marketing_channel_Facebook', 'marketing_channel_Google', ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pandas get_dummies() function\n",
    "df_dummies = pd.get_dummies(df, columns=['gender'], drop_first=True)\n",
    "\n",
    "# Resulting columns: 'gender_M'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using drop_first=True will avoid multicollinearity issues by dropping the first category, which serves as the reference category.\n",
    "\n",
    "#After creating dummy variables, remember to drop the original categorical columns from the DataFrame to prevent redundancy and improve model performance. Also, make sure to incorporate these new features into your data preparation and model building process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Import the dataset\n",
    "df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Data Preparation\n",
    "# ... (Drop irrelevant columns, handle missing values, etc.)\n",
    "\n",
    "# Calculate average time spent for users who converted and didn't convert\n",
    "avg_time_spent_converted = df[df['converted_Fri'] == 1]['total_time_spent_week'].mean()\n",
    "avg_time_spent_not_converted = df[df['converted_Fri'] == 0]['total_time_spent_week'].mean()\n",
    "\n",
    "# Visualize distributions using box plots\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.boxplot([df[df['converted_Fri'] == 1]['total_time_spent_week'],\n",
    "             df[df['converted_Fri'] == 0]['total_time_spent_week']],\n",
    "            labels=['Converted', 'Not Converted'])\n",
    "plt.title('Distribution of Total Time Spent by Conversion Status')\n",
    "plt.ylabel('Total Time Spent')\n",
    "plt.show()\n",
    "\n",
    "# Perform a t-test to test the hypothesis\n",
    "t_stat, p_value = ttest_ind(df[df['converted_Fri'] == 1]['total_time_spent_week'],\n",
    "                            df[df['converted_Fri'] == 0]['total_time_spent_week'])\n",
    "\n",
    "print(\"T-Statistic:\", t_stat)\n",
    "print(\"P-Value:\", p_value)\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"The difference in time spent is statistically significant.\")\n",
    "else:\n",
    "    print(\"There is no statistically significant difference in time spent.\")\n",
    "\n",
    "#based on hypothesis\n",
    "#Users who spend more time on the website throughout the week are more likely to convert on Friday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ttest_ind\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Import the dataset\n",
    "df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Data Preparation\n",
    "# ... (Drop irrelevant columns, handle missing values, etc.)\n",
    "\n",
    "# Calculate average time spent for users who converted and didn't convert\n",
    "avg_time_spent_converted = df[df['converted_Fri'] == 1]['total_time_spent_week'].mean()\n",
    "avg_time_spent_not_converted = df[df['converted_Fri'] == 0]['total_time_spent_week'].mean()\n",
    "\n",
    "# Visualize distributions using box plots\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.boxplot([df[df['converted_Fri'] == 1]['total_time_spent_week'],\n",
    "             df[df['converted_Fri'] == 0]['total_time_spent_week']],\n",
    "            labels=['Converted', 'Not Converted'])\n",
    "plt.title('Distribution of Total Time Spent by Conversion Status')\n",
    "plt.ylabel('Total Time Spent')\n",
    "plt.show()\n",
    "\n",
    "# Perform a t-test to test the hypothesis\n",
    "t_stat, p_value = ttest_ind(df[df['converted_Fri'] == 1]['total_time_spent_week'],\n",
    "                            df[df['converted_Fri'] == 0]['total_time_spent_week'])\n",
    "\n",
    "print(\"T-Statistic:\", t_stat)\n",
    "print(\"P-Value:\", p_value)\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"The difference in time spent is statistically significant.\")\n",
    "else:\n",
    "    print(\"There is no statistically significant difference in time spent.\")\n",
    "\n",
    "# Model Building\n",
    "X = df.drop(['visitor_id', 'converted_Fri'], axis=1)\n",
    "y = df['converted_Fri']\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define preprocessing steps\n",
    "numeric_features = ['product_views_Mon', 'product_views_Tue', 'product_views_Wed', 'product_views_Thu',\n",
    "                    'product_views_Fri', 'time_on_site_Mon', 'time_on_site_Tue', 'time_on_site_Wed',\n",
    "                    'time_on_site_Thu', 'time_on_site_Fri', 'pages_visited_Mon', 'pages_visited_Tue',\n",
    "                    'pages_visited_Wed', 'pages_visited_Thu', 'pages_visited_Fri', 'avg_rating_week', 'avg_price_week']\n",
    "categorical_features = ['visitor_location', 'marketing_channel', 'gender']\n",
    "\n",
    "numeric_transformer = StandardScaler()\n",
    "\n",
    "# ... Define categorical_transformer as in previous code ...\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Build a Random Forest model\n",
    "model = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                        ('classifier', RandomForestClassifier(random_state=42))])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Model Evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Model Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import the dataset\n",
    "df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Data Preparation\n",
    "# ... (Drop irrelevant columns, handle missing values, etc.)\n",
    "\n",
    "# Explore conversion rate by demographic and behavioral variables\n",
    "conversion_by_gender = df.groupby('gender')['converted_Fri'].mean()\n",
    "conversion_by_channel = df.groupby('marketing_channel')['converted_Fri'].mean()\n",
    "\n",
    "# Create a bar plot for conversion rate by gender\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=conversion_by_gender.index, y=conversion_by_gender.values)\n",
    "plt.title('Conversion Rate by Gender')\n",
    "plt.xlabel('Gender')\n",
    "plt.ylabel('Conversion Rate')\n",
    "plt.show()\n",
    "\n",
    "# Create a bar plot for conversion rate by marketing channel\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=conversion_by_channel.index, y=conversion_by_channel.values)\n",
    "plt.title('Conversion Rate by Marketing Channel')\n",
    "plt.xlabel('Marketing Channel')\n",
    "plt.ylabel('Conversion Rate')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import ttest_ind\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Import the dataset\n",
    "df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Data Preparation\n",
    "# ... (Drop irrelevant columns, handle missing values, etc.)\n",
    "\n",
    "# Explore conversion rate by gender\n",
    "conversion_by_gender = df.groupby('gender')['converted_Fri'].mean()\n",
    "\n",
    "# Visualize conversion rate by gender\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=conversion_by_gender.index, y=conversion_by_gender.values)\n",
    "plt.title('Conversion Rate by Gender')\n",
    "plt.xlabel('Gender')\n",
    "plt.ylabel('Conversion Rate')\n",
    "plt.show()\n",
    "\n",
    "# Perform a t-test to test the hypothesis\n",
    "converted_females = df[df['gender'] == 'F']['converted_Fri']\n",
    "converted_males = df[df['gender'] == 'M']['converted_Fri']\n",
    "\n",
    "t_stat, p_value = ttest_ind(converted_females, converted_males)\n",
    "\n",
    "print(\"T-Statistic:\", t_stat)\n",
    "print(\"P-Value:\", p_value)\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"Females are more likely to convert than males.\")\n",
    "else:\n",
    "    print(\"There is no statistically significant difference in conversion rates between genders.\")\n",
    "\n",
    "# Model Building\n",
    "X = df.drop(['visitor_id', 'converted_Fri'], axis=1)\n",
    "y = df['converted_Fri']\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define preprocessing steps\n",
    "numeric_features = ['product_views_Mon', 'product_views_Tue', 'product_views_Wed', 'product_views_Thu',\n",
    "                    'product_views_Fri', 'time_on_site_Mon', 'time_on_site_Tue', 'time_on_site_Wed',\n",
    "                    'time_on_site_Thu', 'time_on_site_Fri', 'pages_visited_Mon', 'pages_visited_Tue',\n",
    "                    'pages_visited_Wed', 'pages_visited_Thu', 'pages_visited_Fri', 'avg_rating_week', 'avg_price_week']\n",
    "categorical_features = ['visitor_location', 'marketing_channel']\n",
    "\n",
    "numeric_transformer = StandardScaler()\n",
    "\n",
    "# ... Define categorical_transformer as in previous code ...\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Build a Random Forest model\n",
    "model = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                        ('classifier', RandomForestClassifier(random_state=42))])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Model Evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Model Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Import the dataset\n",
    "df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Data Preparation\n",
    "# ... (Drop irrelevant columns, handle missing values, etc.)\n",
    "\n",
    "# Model Building\n",
    "X = df.drop(['visitor_id', 'converted_Fri'], axis=1)\n",
    "y = df['converted_Fri']\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define preprocessing steps\n",
    "numeric_features = ['product_views_Mon', 'product_views_Tue', 'product_views_Wed', 'product_views_Thu',\n",
    "                    'product_views_Fri', 'time_on_site_Mon', 'time_on_site_Tue', 'time_on_site_Wed',\n",
    "                    'time_on_site_Thu', 'time_on_site_Fri', 'pages_visited_Mon', 'pages_visited_Tue',\n",
    "                    'pages_visited_Wed', 'pages_visited_Thu', 'pages_visited_Fri', 'avg_rating_week', 'avg_price_week']\n",
    "categorical_features = ['visitor_location', 'marketing_channel', 'gender']\n",
    "\n",
    "numeric_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(drop='first')\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'SVM': SVC(),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "for name, model in models.items():\n",
    "    model_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                     ('classifier', model)])\n",
    "    \n",
    "    # Train the model\n",
    "    model_pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on test data\n",
    "    y_pred = model_pipeline.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Model: {name}\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}, Precision: {precision:.2f}, Recall: {recall:.2f}, F1-Score: {f1:.2f}\")\n",
    "    print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Import the dataset\n",
    "df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Data Exploration\n",
    "# ... (Exploring conversion rates by marketing channels, demographics, etc.)\n",
    "\n",
    "# Prepare Data for Modeling\n",
    "# ... (Data cleaning, feature engineering, creating dummy variables, etc.)\n",
    "\n",
    "# Focus on marketing channels Facebook, Instagram, Google, and YouTube\n",
    "channels_of_interest = ['Facebook', 'Instagram', 'Google', 'YouTube']\n",
    "df['exposed_to_channels'] = df['marketing_channel'].apply(lambda x: 1 if x in channels_of_interest else 0)\n",
    "\n",
    "# Model Building\n",
    "X = df.drop(['visitor_id', 'converted_Fri', 'marketing_channel'], axis=1)\n",
    "y = df['converted_Fri']\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define preprocessing steps\n",
    "numeric_features = ['product_views_Mon', 'product_views_Tue', 'product_views_Wed', 'product_views_Thu',\n",
    "                    'product_views_Fri', 'time_on_site_Mon', 'time_on_site_Tue', 'time_on_site_Wed',\n",
    "                    'time_on_site_Thu', 'time_on_site_Fri', 'pages_visited_Mon', 'pages_visited_Tue',\n",
    "                    'pages_visited_Wed', 'pages_visited_Thu', 'pages_visited_Fri', 'avg_rating_week', 'avg_price_week']\n",
    "categorical_features = ['visitor_location', 'gender']\n",
    "\n",
    "numeric_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(drop='first')\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'SVM': SVC(),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "for name, model in models.items():\n",
    "    model_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                     ('classifier', model)])\n",
    "    \n",
    "    # Train the model\n",
    "    model_pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on test data\n",
    "    y_pred = model_pipeline.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Model: {name}\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}, Precision: {precision:.2f}, Recall: {recall:.2f}, F1-Score: {f1:.2f}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "# Explanation of Results\n",
    "# ... (Explain findings from data exploration, model evaluation, and how the hypothesis is supported or not)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Import the dataset\n",
    "df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Data Preparation\n",
    "# ... (Drop irrelevant columns, handle missing values, etc.)\n",
    "\n",
    "# Feature Engineering\n",
    "channels_of_interest = ['Facebook', 'Instagram', 'Google', 'YouTube']\n",
    "df['exposed_to_channels'] = df['marketing_channel'].apply(lambda x: 1 if x in channels_of_interest else 0)\n",
    "\n",
    "# Data Exploration\n",
    "# Conversion Rate by Exposed Channels\n",
    "conversion_by_channels = df.groupby('exposed_to_channels')['converted_Fri'].mean()\n",
    "\n",
    "# Create a bar plot for conversion rate by exposed channels\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=conversion_by_channels.index, y=conversion_by_channels.values)\n",
    "plt.title('Conversion Rate by Exposed Channels')\n",
    "plt.xlabel('Exposed to Channels')\n",
    "plt.ylabel('Conversion Rate')\n",
    "plt.xticks([0, 1], ['Not Exposed', 'Exposed'])\n",
    "plt.show()\n",
    "\n",
    "# Prepare Data for Modeling\n",
    "# ... (Data cleaning, feature engineering, creating dummy variables, etc.)\n",
    "\n",
    "# Model Building\n",
    "X = df.drop(['visitor_id', 'converted_Fri', 'marketing_channel'], axis=1)\n",
    "y = df['converted_Fri']\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define preprocessing steps\n",
    "numeric_features = ['product_views_Mon', 'product_views_Tue', 'product_views_Wed', 'product_views_Thu',\n",
    "                    'product_views_Fri', 'time_on_site_Mon', 'time_on_site_Tue', 'time_on_site_Wed',\n",
    "                    'time_on_site_Thu', 'time_on_site_Fri', 'pages_visited_Mon', 'pages_visited_Tue',\n",
    "                    'pages_visited_Wed', 'pages_visited_Thu', 'pages_visited_Fri', 'avg_rating_week', 'avg_price_week']\n",
    "categorical_features = ['visitor_location', 'gender']\n",
    "\n",
    "numeric_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(drop='first')\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "for name, model in models.items():\n",
    "    model_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                     ('classifier', model)])\n",
    "    \n",
    "    # Train the model\n",
    "    model_pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on test data\n",
    "    y_pred = model_pipeline.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Model: {name}\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}, Precision: {precision:.2f}, Recall: {recall:.2f}, F1-Score: {f1:.2f}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "# Explanation of Results\n",
    "# ... (Explain findings from data exploration, model evaluation, and how the hypothesis is supported or not)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
